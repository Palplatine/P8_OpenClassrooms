{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe30a7b0",
   "metadata": {},
   "source": [
    "## Imports et création de notre session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2c78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257b361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée notre environnement Spark\n",
    "spark = SparkSession.builder.appName('P8_OCR_VLE').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ab5f1",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7794ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    \"\"\"\n",
    "    Prend le chemin dans un bucket S3 d'un objet (une image), le lit et\n",
    "    le transforme sous la forme d'une liste.\n",
    "\n",
    "            Parameters:\n",
    "                img_path : le chemin de l'image dans notre bucket S3\n",
    "    \"\"\"\n",
    "    \n",
    "    # On récupère nos images en se connectant à notre bucket S3\n",
    "    s3 = boto3.resource('s3', region_name='eu-west-3')\n",
    "    bucket = s3.Bucket('p8ocrvle')\n",
    "    \n",
    "    # On récupère les objets dans notre bucket\n",
    "    img = img_path.replace('s3://p8ocrvle/', '')\n",
    "    bucket_file = bucket.Object(img)\n",
    "    s3_response = bucket_file.get()\n",
    "    file_stream = s3_response['Body']\n",
    "    image = Image.open(file_stream)\n",
    "    \n",
    "    # On enlève les images qui ne retournent rien\n",
    "    if image is None:\n",
    "        image = 0\n",
    "        \n",
    "    else:\n",
    "        image = np.asarray(image)\n",
    "        image = np.resize(image, (299, 299, 3))\n",
    "        image = image.flatten().tolist()\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18333ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pretrained():\n",
    "    \"\"\"\n",
    "    Retourne notre modèle (InceptionV3), sans la couche la plus haute\n",
    "    (couche de classification) et on y ajoute manuellement les poids pré-entraînés.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = InceptionV3(weights=None, include_top=False)\n",
    "    model.set_weights(model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b538e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(data):\n",
    "    \"\"\"\n",
    "    Prend une liste représentative de nos images, les transforme\n",
    "    en array et change le format de l'image. Retourne les images\n",
    "    auxquelles on applique le preprocessing propre à notre modèle.\n",
    "    \"\"\"\n",
    "    \n",
    "    image = np.asarray(data)\n",
    "    image = np.resize(image, (299, 299, 3))\n",
    "\n",
    "    return preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2568e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Récupère toutes nos images pour y appliquer notre preprocessing\n",
    "    et on applique notre modèle, avant de faire une classification.\n",
    "    Retourne une série composée de nos features.\n",
    "    \n",
    "            Parameters:\n",
    "                model : notre modèle (InceptionV3 ici)\n",
    "                content_series : nos données images\n",
    "    \"\"\"\n",
    "    \n",
    "    model_input = np.stack(content_series.map(preprocess_img)) \n",
    "    preds = model.predict(model_input)\n",
    "    model_output = [x.flatten() for x in preds]\n",
    "    return pd.Series(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f39481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:383: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def udf_featurize_series(content_series_iter):\n",
    "    \"\"\"\n",
    "    UDF (User Defined Function) wrapper pour notre fonction de featurisation.\n",
    "    Retourne une colonne de DataFrame Spark de type ArrayType(FloatType)\n",
    "    \n",
    "            Parameters:\n",
    "            content_series_iter : série de nos données images\n",
    "    \"\"\"\n",
    "    model = model_pretrained()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccea28d",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43eaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se connecte à notre bucket S3\n",
    "session = boto3.session.Session(aws_access_key_id='XXX',\n",
    "                                aws_secret_access_key='XXX')\n",
    "\n",
    "s3_client = session.client(service_name='s3', region_name='eu-west-3')\n",
    "\n",
    "# Et on se prépare à lire nos fichiers\n",
    "prefix = 'Data'\n",
    "sub_folders = s3_client.list_objects_v2(Bucket='p8ocrvle', Prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96821dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lit les fichiers présent dans notre dossiers \"Data\"\n",
    "# On récupère l'emplacement des fichiers\n",
    "lst_path = []\n",
    "for key in sub_folders['Contents']:\n",
    "    file = key['Key']\n",
    "    file = file.replace(prefix + '/', '')\n",
    "    lst_path.append('s3://p8ocrvle/Data/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5857cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée notre DataFrame et on y met l'emplacement de fichiers\n",
    "rdd = spark.sparkContext.parallelize(lst_path)\n",
    "row_rdd = rdd.map(lambda x: Row(x))\n",
    "df_pyspark = spark.createDataFrame(row_rdd, ['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1b6a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|path                                           |\n",
      "+-----------------------------------------------+\n",
      "|s3://p8ocrvle/Data/apple_6/r0_4.jpg            |\n",
      "|s3://p8ocrvle/Data/apple_6/r0_6.jpg            |\n",
      "|s3://p8ocrvle/Data/apple_pink_lady_1/r0_32.jpg |\n",
      "|s3://p8ocrvle/Data/apple_pink_lady_1/r0_34.jpg |\n",
      "|s3://p8ocrvle/Data/apple_red_yellow_1/r0_32.jpg|\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On vérifie\n",
    "df_pyspark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "868d0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère nos labels\n",
    "df_pyspark = df_pyspark.withColumn('label', split(col('path'), '/').getItem(4))\n",
    "\n",
    "# On ajoute une colonne avec nos données images\n",
    "udf_image = udf(load_img, ArrayType(IntegerType()))\n",
    "df_pyspark = df_pyspark.withColumn('data', udf_image('path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77ed7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|                path|             label|                data|\n",
      "+--------------------+------------------+--------------------+\n",
      "|s3://p8ocrvle/Dat...|           apple_6|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...|           apple_6|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...| apple_pink_lady_1|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...| apple_pink_lady_1|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...|apple_red_yellow_1|[255, 255, 255, 2...|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On vérifie de nouveau\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c41a44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------+--------------------+\n",
      "|                path|             label|label_encoded|                data|\n",
      "+--------------------+------------------+-------------+--------------------+\n",
      "|s3://p8ocrvle/Dat...|           apple_6|            0|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...|           apple_6|            0|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...| apple_pink_lady_1|            1|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...| apple_pink_lady_1|            1|[255, 255, 255, 2...|\n",
      "|s3://p8ocrvle/Dat...|apple_red_yellow_1|            2|[255, 255, 255, 2...|\n",
      "+--------------------+------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On va utiliser un StringIndexer pour encoder nos labels\n",
    "stringIndexer = StringIndexer(inputCol='label', outputCol='label_encoded')\n",
    "sI = stringIndexer.fit(df_pyspark)\n",
    "\n",
    "# On encode et on convertit nos labels en Integer (lisibilité)\n",
    "image_df = sI.transform(df_pyspark)\n",
    "image_df = image_df.withColumn('label_encoded', col('label_encoded').cast(IntegerType()))\n",
    "\n",
    "# On réorganise nos colonnes (lisibilité)\n",
    "image_df = image_df.select('path', 'label', 'label_encoded', 'data')\n",
    "image_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7b79d",
   "metadata": {},
   "source": [
    "## Preprocessing de nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef623310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge notre model et les poids associés\n",
    "model = InceptionV3(include_top=False)\n",
    "model_weights = spark.sparkContext.broadcast(model.get_weights()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15759b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique notre preprocessing\n",
    "features_df = image_df.select(col('path'), col('label'), col('label_encoded'),\n",
    "                              udf_featurize_series('data').alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a1d8e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------+--------------------+\n",
      "|                path|             label|label_encoded|            features|\n",
      "+--------------------+------------------+-------------+--------------------+\n",
      "|s3://p8ocrvle/Dat...|           apple_6|            0|[0.0, 0.0, 0.0, 0...|\n",
      "|s3://p8ocrvle/Dat...|           apple_6|            0|[0.0, 0.0665489, ...|\n",
      "|s3://p8ocrvle/Dat...| apple_pink_lady_1|            1|[0.0, 0.0, 0.0, 0...|\n",
      "|s3://p8ocrvle/Dat...| apple_pink_lady_1|            1|[0.0, 0.0, 0.0, 0...|\n",
      "|s3://p8ocrvle/Dat...|apple_red_yellow_1|            2|[0.025733098, 0.0...|\n",
      "+--------------------+------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On vérifie\n",
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a87c0",
   "metadata": {},
   "source": [
    "## Réduction de dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb372b4b",
   "metadata": {},
   "source": [
    "Avant d'appliquer une PCA, nous devons centrer et réduire nos données, nous allons donc appliquer le StandardScaler de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb8e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme nos données en vecteurs\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "features_df = features_df.select(col('path'),  col('label'),\n",
    "                                 list_to_vector_udf(features_df['features']).alias('features_vect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49c62dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features_vect: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On vérifie\n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "049c9b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On standardise nos données\n",
    "standardizer = StandardScaler(withMean=False, withStd=True,\n",
    "                              inputCol='features_vect',\n",
    "                              outputCol='feats_scaled')\n",
    "std = standardizer.fit(features_df)\n",
    "features_df_scaled = std.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie\n",
    "features_df_scaled.printSchema()\n",
    "features_df_scaled.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212cee4",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant appliquer une PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c38715",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=8, inputCol='feats_scaled', outputCol='pca')\n",
    "modelpca = pca.fit(features_df_scaled)\n",
    "transformed = modelpca.transform(features_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde la variance expliquée par nos nouvelles données\n",
    "variance_explained = modelpca.explainedVariance\n",
    "variance_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc961f",
   "metadata": {},
   "source": [
    "## Enregistrement des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd155b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre dans un nouveau dossier dans notre bucket S3\n",
    "features_df_scaled.write.parquet(path='s3://p8ocrvle/Features/', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
