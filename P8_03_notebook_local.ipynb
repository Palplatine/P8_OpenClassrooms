{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ddab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b4e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée notre environnement Spark\n",
    "spark = SparkSession.builder.appName('P8_OCR_VLE').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c960b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On règle quelques paramètres de configuration\n",
    "# spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "# spark.conf.set('spark.sql.execution.arrow.maxRecordsPerBatch', '1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac9a497e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-KG300EE6:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>P8_OCR_VLE</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23a97c5bd60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On vérifie\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545c7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée notre dataframe\n",
    "images_dir = 'P8_data_sample/Data'\n",
    "df_pyspark = spark.read.format('binaryFile').option('recursiveFileLookup', 'true')\\\n",
    "            .load(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6fba988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, modificationTime: timestamp, length: bigint, content: binary]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On regarde nos colonnes et leur type\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b72f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schéma de notre dataframe\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61e2310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|file:/C:/Users/vi...|2022-07-21 15:02:...| 38050|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/vi...|2022-07-21 15:02:...| 37757|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/vi...|2022-07-21 15:03:...| 33241|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/vi...|2022-07-21 15:03:...| 33175|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/vi...|2022-07-21 15:01:...| 16421|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nos premières lignes\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f55e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+-------------+\n",
      "|                path|             content|             label|label_encoded|\n",
      "+--------------------+--------------------+------------------+-------------+\n",
      "|file:/C:/Users/vi...|[FF D8 FF E0 00 1...| apple_pink_lady_1|            1|\n",
      "|file:/C:/Users/vi...|[FF D8 FF E0 00 1...| apple_pink_lady_1|            1|\n",
      "|file:/C:/Users/vi...|[FF D8 FF E0 00 1...|apple_red_yellow_1|            2|\n",
      "|file:/C:/Users/vi...|[FF D8 FF E0 00 1...|apple_red_yellow_1|            2|\n",
      "|file:/C:/Users/vi...|[FF D8 FF E0 00 1...|           apple_6|            0|\n",
      "|file:/C:/Users/vi...|[FF D8 FF E0 00 1...|           apple_6|            0|\n",
      "+--------------------+--------------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On crée une colonne pour avoir nos labels et une colonne avec les labels encodés\n",
    "df_pyspark = df_pyspark.withColumn('label', split(col('path'), '/').getItem(6))\n",
    "\n",
    "# On va utiliser un StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol='label', outputCol='label_encoded')\n",
    "sI = stringIndexer.fit(df_pyspark)\n",
    "\n",
    "# On encode et on convertit nos labels en Integer par soucis de lisibilité\n",
    "image_df = sI.transform(df_pyspark)\n",
    "image_df = image_df.withColumn('label_encoded', col('label_encoded').cast(IntegerType()))\n",
    "\n",
    "# On ne garde que quelques colonnes\n",
    "image_df = image_df.select('path', 'content', 'label', 'label_encoded')\n",
    "image_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f5567",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2dd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08d546bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_model_weights = spark.sparkContext.broadcast(model.get_weights()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f3ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = ResNet50(weights=None, include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26573227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a581b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    \n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    \n",
    "    input = np.stack(content_series.map(preprocess)) \n",
    "    preds = model.predict(input)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc750eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:383: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    \"\"\"\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                                is a pandas Series of image data.\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e77114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = image_df.select(col('path'), col('label'), featurize_udf('content').alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88750de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e86837",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|                path|             label|            features|\n",
      "+--------------------+------------------+--------------------+\n",
      "|file:/C:/Users/vi...| apple_pink_lady_1|[0.0, 0.0, 0.0, 0...|\n",
      "|file:/C:/Users/vi...| apple_pink_lady_1|[0.0, 0.0, 0.0, 0...|\n",
      "|file:/C:/Users/vi...|apple_red_yellow_1|[0.0, 0.0, 0.0, 0...|\n",
      "|file:/C:/Users/vi...|apple_red_yellow_1|[0.0, 0.0, 0.0, 0...|\n",
      "|file:/C:/Users/vi...|           apple_6|[0.0, 0.0, 0.0, 0...|\n",
      "|file:/C:/Users/vi...|           apple_6|[0.0, 0.0, 0.0, 0...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37aa609",
   "metadata": {},
   "source": [
    "## Scaling des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec6fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "features_df = features_df.select(col('path'),  col('label'), list_to_vector_udf(features_df['features']).alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ea69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler(withMean=True, withStd=True,\n",
    "                              inputCol='features',\n",
    "                              outputCol='feats_scaled')\n",
    "std = standardizer.fit(features_df)\n",
    "features_df_scaled = std.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc8d1dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- feats_scaled: vector (nullable = true)\n",
      "\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|                path|             label|            features|        feats_scaled|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|file:/C:/Users/vi...| apple_pink_lady_1|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|file:/C:/Users/vi...| apple_pink_lady_1|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|file:/C:/Users/vi...|apple_red_yellow_1|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|file:/C:/Users/vi...|apple_red_yellow_1|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|file:/C:/Users/vi...|           apple_6|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "|file:/C:/Users/vi...|           apple_6|[0.0,0.0,0.0,0.0,...|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df_scaled.printSchema()\n",
    "features_df_scaled.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47aac09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = features_df_scaled.select('feats_scaled').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29011ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18597\n"
     ]
    }
   ],
   "source": [
    "num_values = len(set(test[0]))\n",
    "print(num_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5d0339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = set(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dd6c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0412414523193156\n"
     ]
    }
   ],
   "source": [
    "print(max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3d795",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40c8d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=8, inputCol='feats_scaled', outputCol='pca')\n",
    "modelpca = pca.fit(features_df_scaled)\n",
    "transformed = modelpca.transform(features_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d71e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- feats_scaled: vector (nullable = true)\n",
      " |-- pca: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d7082e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.4092, 0.3185, 0.1235, 0.0825, 0.0663])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8\n",
    "variance_explained = modelpca.explainedVariance\n",
    "variance_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fada0d8",
   "metadata": {},
   "source": [
    "## Enregistrement des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09095313",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df_scaled.write.parquet(path='C:/Users/victo/P8_data_sample/Features', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95142e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f06d8e48a74e9c77d2c6255e2acc8075fc17268ff6f9a73f1f3bca0b03277241"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
